#!/bin/bash

# Change to project root directory (one level up from scripts/)
cd "$(dirname "$0")/.." || exit 1

# HuggingFace Token configuration
# Token set for faster downloads and higher API limits
export HF_TOKEN="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"




# CUDA device configuration
# Usage: CUDA_VISIBLE_DEVICES=0,1,2,3 ./script.sh (to use GPUs 0-3)
# Or set CUDA_VISIBLE_DEVICES environment variable before running
export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0,1,2,3,4,5,6,7}
# Default parameters
BATCH_SIZE=${BATCH_SIZE:-15}
GRAD_ACC=${GRAD_ACC:-4}
GPUS=${GPUS:-8}
# Global batch size to maintain (for auto-calculation of GRAD_ACC)
GLOBAL_BATCH_SIZE=${GLOBAL_BATCH_SIZE:-480}

# Auto-calculate GRAD_ACC if GRAD_ACC is default value and GPUS differs from 8
if [ "$GRAD_ACC" == "4" ] && [ "$GPUS" != "8" ]; then
  GRAD_ACC=$((GLOBAL_BATCH_SIZE / (BATCH_SIZE * GPUS)))
  AUTO_CALC=true
else
  AUTO_CALC=false
fi

LR=${LR:-3e-3}
MUON_LR=${MUON_LR:-2e-2}
BETA1=${BETA1:-0.9}
BETA2=${BETA2:-0.95}
WD=${WD:-1e-1}
MUON_WD=${MUON_WD:-0.0}
MAX_ITERS=${MAX_ITERS:-100000}
WARMUP=${WARMUP:-2000}
GRAD_CLIP=${GRAD_CLIP:-1.0}
STREAMING_TIMEOUT=${STREAMING_TIMEOUT:-7200}
STREAMING_RETRIES=${STREAMING_RETRIES:-10}
# Dataset: "karpathy/fineweb-edu-100b-shuffle" or "Skylion007/openwebtext"
DATASET=${DATASET:-"karpathy/fineweb-edu-100b-shuffle"}
# Output directory
OUTPUT_DIR="Output"
mkdir -p ${OUTPUT_DIR}
# Generate run name with key parameters and dataset
DATASET_SHORT=$(echo ${DATASET} | sed 's/.*\///g' | sed 's/-.*//g')  # Extract dataset short name
RUN_NAME="muon-small-${DATASET_SHORT}-lr${LR}-mlr${MUON_LR}-b1_${BETA1}-b2_${BETA2}-wd${WD}-mwd${MUON_WD}-it${MAX_ITERS}"
# Log file
LOG_FILE="${OUTPUT_DIR}/${RUN_NAME}_$(date +%Y%m%d_%H%M%S).log"
echo "Starting training with the following parameters:" | tee ${LOG_FILE}
if [ "$AUTO_CALC" == "true" ]; then
  echo "  [AUTO-CALCULATED] GRAD_ACC_STEPS: ${GRAD_ACC} (to maintain global batch size = ${GLOBAL_BATCH_SIZE})" | tee -a ${LOG_FILE}
fi
echo "  BATCH_SIZE: ${BATCH_SIZE}" | tee -a ${LOG_FILE}
echo "  GRAD_ACC_STEPS: ${GRAD_ACC}" | tee -a ${LOG_FILE}
echo "  NUM_GPUS: ${GPUS}" | tee -a ${LOG_FILE}
echo "  GLOBAL_BATCH_SIZE: $((BATCH_SIZE * GPUS * GRAD_ACC))" | tee -a ${LOG_FILE}
echo "  CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES}" | tee -a ${LOG_FILE}
echo "  LR: ${LR}" | tee -a ${LOG_FILE}
echo "  MUON_LR: ${MUON_LR}" | tee -a ${LOG_FILE}
echo "  BETA1: ${BETA1}" | tee -a ${LOG_FILE}
echo "  BETA2: ${BETA2}" | tee -a ${LOG_FILE}
echo "  WEIGHT_DECAY: ${WD}" | tee -a ${LOG_FILE}
echo "  MUON_WEIGHT_DECAY: ${MUON_WD}" | tee -a ${LOG_FILE}
echo "  MAX_ITERS: ${MAX_ITERS}" | tee -a ${LOG_FILE}
echo "  WARMUP_ITERS: ${WARMUP}" | tee -a ${LOG_FILE}
echo "  GRAD_CLIP: ${GRAD_CLIP}" | tee -a ${LOG_FILE}
echo "  STREAMING_TIMEOUT: ${STREAMING_TIMEOUT}" | tee -a ${LOG_FILE}
echo "  STREAMING_RETRIES: ${STREAMING_RETRIES}" | tee -a ${LOG_FILE}
echo "  DATASET: ${DATASET}" | tee -a ${LOG_FILE}
echo "  RUN_NAME: ${RUN_NAME}" | tee -a ${LOG_FILE}
echo "  LOG_FILE: ${LOG_FILE}" | tee -a ${LOG_FILE}
echo "" | tee -a ${LOG_FILE}
# Run training
torchrun --standalone --nproc_per_node=${GPUS} \
      MARS/train_muon_streaming_fw.py \
      config/train_gpt2_small_muon_streaming_fw.py \
      --batch_size=${BATCH_SIZE} \
      --gradient_accumulation_steps=${GRAD_ACC} \
      --learning_rate=${LR} \
      --muon_learning_rate=${MUON_LR} \
      --beta1=${BETA1} \
      --beta2=${BETA2} \
      --weight_decay=${WD} \
      --muon_weight_decay=${MUON_WD} \
      --max_iters=${MAX_ITERS} \
      --warmup_iters=${WARMUP} \
      --grad_clip=${GRAD_CLIP} \
      --streaming_timeout=${STREAMING_TIMEOUT} \
      --streaming_max_retries=${STREAMING_RETRIES} \
      --streaming_dataset=${DATASET} \
      --wandb_run_name=${RUN_NAME} \
      2>&1 | tee -a ${LOG_FILE}
echo "" | tee -a ${LOG_FILE}
echo "Training completed. Log saved to: ${LOG_FILE}" | tee -a ${LOG_FILE}